{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.ion()  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from keras.optimizers import Adam\n",
    "import cv2 \n",
    "import os\n",
    "import csv\n",
    "from stn import spatial_transformer_network as transformer\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(s):\n",
    "    number=[]\n",
    "    realnumber=[]\n",
    "    for x in os.listdir(s):\n",
    "        for y in os.listdir(s+'/'+x):\n",
    "            im=cv2.imread(s+'/'+x+'/'+y,0)\n",
    "            number.append(im/255)\n",
    "            realnumber.append(int(x))\n",
    "    return np.array(number),np.array(realnumber)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7490, 28, 28, 1)\n",
      "(7490,)\n"
     ]
    }
   ],
   "source": [
    "number , realnumber = load('trainData')\n",
    "number = np.ravel(number).reshape((7490,28, 28,1))\n",
    "print(number.shape)\n",
    "print(realnumber.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1873, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "traind,testd,ytrain,ytest  = train_test_split(number,realnumber,test_size=0.25,random_state=1) \n",
    "print(testd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(10 * 3 * 3, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.zero_()\n",
    "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * 3 * 3)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (localization): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): ReLU(inplace)\n",
       "  )\n",
       "  (fc_loc): Sequential(\n",
       "    (0): Linear(in_features=90, out_features=32, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) torch.float32\n",
      "tensor([8]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "datasetv=[]\n",
    "for z in range(len(train)):\n",
    "    temp=[]\n",
    "    datasetsx = torch.FloatTensor(np.array([train[z]]).reshape(1,1,28,28))\n",
    "    temp.append(datasetsx)\n",
    "    \n",
    "    datasetsy = torch.LongTensor(np.array([ytrain[z]]))\n",
    "    temp.append(datasetsy)\n",
    "    datasetv.append(temp)\n",
    "    \n",
    "for x,y in datasetv:\n",
    "    print(x.shape, x.dtype)\n",
    "    print(y, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) torch.float32\n",
      "torch.Size([1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "datasett=[]\n",
    "for z in range(len(test)):\n",
    "    temp=[]\n",
    "    datasetsxt = torch.FloatTensor(np.array([test[z]]).reshape(1,1,28,28))\n",
    "    temp.append(datasetsxt)\n",
    "    \n",
    "    datasetsyt = torch.LongTensor(np.array([ytest[z]]))\n",
    "    temp.append(datasetsyt)\n",
    "    datasett.append(temp)\n",
    "    \n",
    "for x,y in datasett:\n",
    "    print(x.shape, x.dtype)\n",
    "    print(y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx,(data, target) in enumerate(datasetv):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 500 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(datasetv),\n",
    "                100. * batch_idx / len(datasetv), loss.item()))\n",
    "\n",
    "# A simple test procedure to measure STN the performances on MNIST.\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in datasett:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= 1873\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "              .format(test_loss, correct, 1873,\n",
    "                      100. * correct / 1873))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the STN results\n",
    "---------------------------\n",
    "\n",
    "Now, we will inspect the results of our learned visual attention\n",
    "mechanism.\n",
    "\n",
    "We define a small helper function in order to visualize the\n",
    "transformations while training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5617 (0%)]\tLoss: 2.515848\n",
      "Train Epoch: 1 [500/5617 (9%)]\tLoss: 2.053609\n",
      "Train Epoch: 1 [1000/5617 (18%)]\tLoss: 1.751445\n",
      "Train Epoch: 1 [1500/5617 (27%)]\tLoss: 1.681979\n",
      "Train Epoch: 1 [2000/5617 (36%)]\tLoss: 2.466562\n",
      "Train Epoch: 1 [2500/5617 (45%)]\tLoss: 1.355245\n",
      "Train Epoch: 1 [3000/5617 (53%)]\tLoss: 2.625295\n",
      "Train Epoch: 1 [3500/5617 (62%)]\tLoss: 2.130863\n",
      "Train Epoch: 1 [4000/5617 (71%)]\tLoss: 2.237971\n",
      "Train Epoch: 1 [4500/5617 (80%)]\tLoss: 0.366557\n",
      "Train Epoch: 1 [5000/5617 (89%)]\tLoss: 0.157020\n",
      "Train Epoch: 1 [5500/5617 (98%)]\tLoss: 0.359852\n",
      "\n",
      "Test set: Average loss: 1.2546, Accuracy: 1016/1873 (54%)\n",
      "\n",
      "Train Epoch: 2 [0/5617 (0%)]\tLoss: 1.364073\n",
      "Train Epoch: 2 [500/5617 (9%)]\tLoss: 0.181174\n",
      "Train Epoch: 2 [1000/5617 (18%)]\tLoss: 0.550606\n",
      "Train Epoch: 2 [1500/5617 (27%)]\tLoss: 1.219063\n",
      "Train Epoch: 2 [2000/5617 (36%)]\tLoss: 1.758215\n",
      "Train Epoch: 2 [2500/5617 (45%)]\tLoss: 0.435990\n",
      "Train Epoch: 2 [3000/5617 (53%)]\tLoss: 2.517928\n",
      "Train Epoch: 2 [3500/5617 (62%)]\tLoss: 0.497021\n",
      "Train Epoch: 2 [4000/5617 (71%)]\tLoss: 0.794923\n",
      "Train Epoch: 2 [4500/5617 (80%)]\tLoss: 0.221663\n",
      "Train Epoch: 2 [5000/5617 (89%)]\tLoss: 8.144063\n",
      "Train Epoch: 2 [5500/5617 (98%)]\tLoss: 0.072166\n",
      "\n",
      "Test set: Average loss: 0.7781, Accuracy: 1398/1873 (75%)\n",
      "\n",
      "Train Epoch: 3 [0/5617 (0%)]\tLoss: 1.594437\n",
      "Train Epoch: 3 [500/5617 (9%)]\tLoss: 0.706612\n",
      "Train Epoch: 3 [1000/5617 (18%)]\tLoss: 0.003715\n",
      "Train Epoch: 3 [1500/5617 (27%)]\tLoss: 0.740298\n",
      "Train Epoch: 3 [2000/5617 (36%)]\tLoss: 1.562670\n",
      "Train Epoch: 3 [2500/5617 (45%)]\tLoss: 0.007781\n",
      "Train Epoch: 3 [3000/5617 (53%)]\tLoss: 0.265611\n",
      "Train Epoch: 3 [3500/5617 (62%)]\tLoss: 0.413962\n",
      "Train Epoch: 3 [4000/5617 (71%)]\tLoss: 0.069435\n",
      "Train Epoch: 3 [4500/5617 (80%)]\tLoss: 0.050211\n",
      "Train Epoch: 3 [5000/5617 (89%)]\tLoss: 2.410465\n",
      "Train Epoch: 3 [5500/5617 (98%)]\tLoss: 0.204309\n",
      "\n",
      "Test set: Average loss: 0.4995, Accuracy: 1625/1873 (87%)\n",
      "\n",
      "Train Epoch: 4 [0/5617 (0%)]\tLoss: 1.481043\n",
      "Train Epoch: 4 [500/5617 (9%)]\tLoss: 1.396473\n",
      "Train Epoch: 4 [1000/5617 (18%)]\tLoss: 0.318504\n",
      "Train Epoch: 4 [1500/5617 (27%)]\tLoss: 0.261765\n",
      "Train Epoch: 4 [2000/5617 (36%)]\tLoss: 0.023777\n",
      "Train Epoch: 4 [2500/5617 (45%)]\tLoss: 0.003947\n",
      "Train Epoch: 4 [3000/5617 (53%)]\tLoss: 0.020153\n",
      "Train Epoch: 4 [3500/5617 (62%)]\tLoss: 0.591810\n",
      "Train Epoch: 4 [4000/5617 (71%)]\tLoss: 0.002969\n",
      "Train Epoch: 4 [4500/5617 (80%)]\tLoss: 0.257016\n",
      "Train Epoch: 4 [5000/5617 (89%)]\tLoss: 0.007174\n",
      "Train Epoch: 4 [5500/5617 (98%)]\tLoss: 0.034386\n",
      "\n",
      "Test set: Average loss: 0.2779, Accuracy: 1731/1873 (92%)\n",
      "\n",
      "Train Epoch: 5 [0/5617 (0%)]\tLoss: 0.040125\n",
      "Train Epoch: 5 [500/5617 (9%)]\tLoss: 0.324302\n",
      "Train Epoch: 5 [1000/5617 (18%)]\tLoss: 0.049297\n",
      "Train Epoch: 5 [1500/5617 (27%)]\tLoss: 0.392340\n",
      "Train Epoch: 5 [2000/5617 (36%)]\tLoss: 2.975246\n",
      "Train Epoch: 5 [2500/5617 (45%)]\tLoss: 0.002396\n",
      "Train Epoch: 5 [3000/5617 (53%)]\tLoss: 0.144288\n",
      "Train Epoch: 5 [3500/5617 (62%)]\tLoss: 0.008705\n",
      "Train Epoch: 5 [4000/5617 (71%)]\tLoss: 0.000185\n",
      "Train Epoch: 5 [4500/5617 (80%)]\tLoss: 0.004177\n",
      "Train Epoch: 5 [5000/5617 (89%)]\tLoss: 0.039548\n",
      "Train Epoch: 5 [5500/5617 (98%)]\tLoss: 0.017022\n",
      "\n",
      "Test set: Average loss: 0.2082, Accuracy: 1756/1873 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/5617 (0%)]\tLoss: 0.524760\n",
      "Train Epoch: 6 [500/5617 (9%)]\tLoss: 0.224211\n",
      "Train Epoch: 6 [1000/5617 (18%)]\tLoss: 0.004166\n",
      "Train Epoch: 6 [1500/5617 (27%)]\tLoss: 0.787865\n",
      "Train Epoch: 6 [2000/5617 (36%)]\tLoss: 0.635151\n",
      "Train Epoch: 6 [2500/5617 (45%)]\tLoss: 0.031726\n",
      "Train Epoch: 6 [3000/5617 (53%)]\tLoss: 0.011522\n",
      "Train Epoch: 6 [3500/5617 (62%)]\tLoss: 0.602854\n",
      "Train Epoch: 6 [4000/5617 (71%)]\tLoss: 0.041707\n",
      "Train Epoch: 6 [4500/5617 (80%)]\tLoss: 0.002152\n",
      "Train Epoch: 6 [5000/5617 (89%)]\tLoss: 0.000185\n",
      "Train Epoch: 6 [5500/5617 (98%)]\tLoss: 0.431956\n",
      "\n",
      "Test set: Average loss: 0.1531, Accuracy: 1797/1873 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/5617 (0%)]\tLoss: 0.539070\n",
      "Train Epoch: 7 [500/5617 (9%)]\tLoss: 0.235641\n",
      "Train Epoch: 7 [1000/5617 (18%)]\tLoss: 0.006831\n",
      "Train Epoch: 7 [1500/5617 (27%)]\tLoss: 0.001574\n",
      "Train Epoch: 7 [2000/5617 (36%)]\tLoss: 0.462824\n",
      "Train Epoch: 7 [2500/5617 (45%)]\tLoss: 0.062203\n",
      "Train Epoch: 7 [3000/5617 (53%)]\tLoss: 0.065053\n",
      "Train Epoch: 7 [3500/5617 (62%)]\tLoss: 0.055857\n",
      "Train Epoch: 7 [4000/5617 (71%)]\tLoss: 0.082981\n",
      "Train Epoch: 7 [4500/5617 (80%)]\tLoss: 0.546467\n",
      "Train Epoch: 7 [5000/5617 (89%)]\tLoss: 0.402314\n",
      "Train Epoch: 7 [5500/5617 (98%)]\tLoss: 0.250887\n",
      "\n",
      "Test set: Average loss: 0.5193, Accuracy: 1562/1873 (83%)\n",
      "\n",
      "Train Epoch: 8 [0/5617 (0%)]\tLoss: 0.005874\n",
      "Train Epoch: 8 [500/5617 (9%)]\tLoss: 0.286680\n",
      "Train Epoch: 8 [1000/5617 (18%)]\tLoss: 0.350514\n",
      "Train Epoch: 8 [1500/5617 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [2000/5617 (36%)]\tLoss: 1.630676\n",
      "Train Epoch: 8 [2500/5617 (45%)]\tLoss: 0.004810\n",
      "Train Epoch: 8 [3000/5617 (53%)]\tLoss: 0.006371\n",
      "Train Epoch: 8 [3500/5617 (62%)]\tLoss: 0.180448\n",
      "Train Epoch: 8 [4000/5617 (71%)]\tLoss: 0.000370\n",
      "Train Epoch: 8 [4500/5617 (80%)]\tLoss: 0.000016\n",
      "Train Epoch: 8 [5000/5617 (89%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [5500/5617 (98%)]\tLoss: 0.066370\n",
      "\n",
      "Test set: Average loss: 0.0929, Accuracy: 1833/1873 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/5617 (0%)]\tLoss: 0.000390\n",
      "Train Epoch: 9 [500/5617 (9%)]\tLoss: 0.098184\n",
      "Train Epoch: 9 [1000/5617 (18%)]\tLoss: 0.002249\n",
      "Train Epoch: 9 [1500/5617 (27%)]\tLoss: 0.000768\n",
      "Train Epoch: 9 [2000/5617 (36%)]\tLoss: 0.037345\n",
      "Train Epoch: 9 [2500/5617 (45%)]\tLoss: 0.000026\n",
      "Train Epoch: 9 [3000/5617 (53%)]\tLoss: 0.699576\n",
      "Train Epoch: 9 [3500/5617 (62%)]\tLoss: 0.387007\n",
      "Train Epoch: 9 [4000/5617 (71%)]\tLoss: 0.003175\n",
      "Train Epoch: 9 [4500/5617 (80%)]\tLoss: 0.000042\n",
      "Train Epoch: 9 [5000/5617 (89%)]\tLoss: 0.000626\n",
      "Train Epoch: 9 [5500/5617 (98%)]\tLoss: 0.496528\n",
      "\n",
      "Test set: Average loss: 0.1475, Accuracy: 1818/1873 (97%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_image_np(inp):\n",
    "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "# We want to visualize the output of the spatial transformers layer\n",
    "# after the training, we visualize a batch of input images and\n",
    "# the corresponding transformed batch using STN.\n",
    "\n",
    "\n",
    "def visualize_stn():\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of training data\n",
    "        data = next(iter(datasett))[0].to(device)\n",
    "\n",
    "        input_tensor = data.cpu()\n",
    "        transformed_input_tensor = model.stn(data).cpu()\n",
    "\n",
    "        in_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(input_tensor))\n",
    "\n",
    "        out_grid = convert_image_np(\n",
    "            torchvision.utils.make_grid(transformed_input_tensor))\n",
    "\n",
    "        # Plot the results side-by-side\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(in_grid)\n",
    "        axarr[0].set_title('Dataset Images')\n",
    "\n",
    "        axarr[1].imshow(out_grid)\n",
    "        axarr[1].set_title('Transformed Images')\n",
    "\n",
    "for epoch in range(1,10):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "# Visualize the STN transformation on some input batch\n",
    "visualize_stn()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = model(torch.FloatTensor(np.array([traind[1]]).reshape(1,1,28,28)))\n",
    "plt.imshow(traind[1].reshape(28,28))\n",
    "plt.show()\n",
    "print(np.argmax(x.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2510, 28, 28, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='testData/DigitTest'\n",
    "name=[]\n",
    "immatrix=[]\n",
    "for x in os.listdir(s):\n",
    "    im=cv2.imread(s+'/'+x,0)\n",
    "    immatrix.append(im/256)\n",
    "    name.append(os.path.splitext(x)[0])\n",
    "test=np.array(immatrix).reshape((2510, 28, 28,1))\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "ll=[]\n",
    "for x in test:\n",
    "    iki = model(torch.FloatTensor(np.array([x]).reshape(1,1,28,28)))\n",
    "    l=(np.argmax(iki.detach().numpy()))\n",
    "    ll.append(l)\n",
    "print(ll[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictall.csv', mode='w') as pre:\n",
    "    pre_writer = csv.writer(pre, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    pre_writer.writerow(['Id', 'Category'])\n",
    "    for x in range(len(ll)):\n",
    "        pre_writer.writerow([ll[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = np.array([[1., 0, 0], [0, 1., 0]])\n",
    "initial = initial.astype('float32').flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
