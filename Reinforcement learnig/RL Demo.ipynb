{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rat Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rat: \n",
    "    '''\n",
    "    left=0, right=1, up=2, down=3\n",
    "    states = 0,1,2,3,4,5\n",
    "    4=-10 reward death\n",
    "    5=10 reward\n",
    "    1=1 reward\n",
    "    2=2 reward\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.transition = [[0,0,1,3,4,5],[1,2,2,4,4,5],[0,1,2,0,4,5],[3,4,5,3,4,5]]\n",
    "        self.initState = 0\n",
    "        self.currState = 0\n",
    "        self.rewardTable = [0,1,0,2,-10,10]\n",
    "        self.stateSpace = 6\n",
    "        self.actionSpace = 4\n",
    "        return;\n",
    "    \n",
    "    def step(self,action):\n",
    "        newState = self.transition[action][self.currState]\n",
    "        if newState == self.currState:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = self.rewardTable[newState]\n",
    "        self.currState = newState\n",
    "        isDone = False\n",
    "        if self.currState == 4 or self.currState == 5:\n",
    "            isDone = True\n",
    "        return (newState,reward,isDone)\n",
    "    \n",
    "    def reset(self,):\n",
    "        self.initState = 0\n",
    "        self.currState = 0\n",
    "        return self.currState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning in Rat env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 0.0\n",
      "New Q( ( 0 , 0 ) , 0 ) = 0.0 + 0.81 * ( 0.0 - 0.0 ) = 0.0\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 0.0\n",
      "New Q( ( 0 , 0 ) , 1 ) = 0.0 + 0.81 * ( 1.0 - 0.0 ) = 0.81\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 0 , 1 ) , 3 ) = 0.0\n",
      "New Q( ( 0 , 1 ) , 3 ) = 0.0 + 0.81 * ( -10.0 - 0.0 ) = -8.1\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 0.0\n",
      "New Q( ( 0 , 0 ) , 0 ) = 0.0 + 0.81 * ( 0.7776 - 0.0 ) = 0.63\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 0.63\n",
      "New Q( ( 0 , 0 ) , 0 ) = 0.62986 + 0.81 * ( 0.7776 - 0.62986 ) = 0.75\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 0.75\n",
      "New Q( ( 0 , 0 ) , 0 ) = 0.74953 + 0.81 * ( 0.7776 - 0.74953 ) = 0.77\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 0.0\n",
      "New Q( ( 0 , 0 ) , 2 ) = 0.0 + 0.81 * ( 0.7776 - 0.0 ) = 0.63\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 0.81\n",
      "New Q( ( 0 , 0 ) , 1 ) = 0.81 + 0.81 * ( 1.0 - 0.81 ) = 0.96\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 1 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 2 ) = 0.0\n",
      "New Q( ( 0 , 1 ) , 2 ) = 0.0 + 0.81 * ( 0.0 - 0.0 ) = 0.0\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 1 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 2 ) = 0.0\n",
      "New Q( ( 0 , 1 ) , 2 ) = 0.0 + 0.81 * ( 0.0 - 0.0 ) = 0.0\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 0 ) = 0.0\n",
      "New Q( ( 0 , 1 ) , 0 ) = 0.0 + 0.81 * ( 0.92534 - 0.0 ) = 0.75\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 0.63\n",
      "New Q( ( 0 , 0 ) , 2 ) = 0.62986 + 0.81 * ( 0.92534 - 0.62986 ) = 0.87\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 0.0\n",
      "New Q( ( 0 , 0 ) , 3 ) = 0.0 + 0.81 * ( 2.0 - 0.0 ) = 1.62\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 0.0\n",
      "New Q( ( 1 , 0 ) , 3 ) = 0.0 + 0.81 * ( 0.0 - 0.0 ) = 0.0\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 0.0\n",
      "New Q( ( 1 , 0 ) , 2 ) = 0.0 + 0.81 * ( 1.5552 - 0.0 ) = 1.26\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 1.62\n",
      "New Q( ( 0 , 0 ) , 3 ) = 1.62 + 0.81 * ( 3.20932 - 1.62 ) = 2.91\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 1.26\n",
      "New Q( ( 1 , 0 ) , 2 ) = 1.25971 + 0.81 * ( 2.79106 - 1.25971 ) = 2.5\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 2.91\n",
      "New Q( ( 0 , 0 ) , 3 ) = 2.90735 + 0.81 * ( 4.4001 - 2.90735 ) = 4.12\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 1 , 0 ) , 1 ) = 0.0\n",
      "New Q( ( 1 , 0 ) , 1 ) = 0.0 + 0.81 * ( -10.0 - 0.0 ) = -8.1\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 4.12\n",
      "New Q( ( 0 , 0 ) , 3 ) = 4.11648 + 0.81 * ( 4.4001 - 4.11648 ) = 4.35\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 2.5\n",
      "New Q( ( 1 , 0 ) , 2 ) = 2.5001 + 0.81 * ( 4.17236 - 2.5001 ) = 3.85\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 0.77\n",
      "New Q( ( 0 , 0 ) , 0 ) = 0.77227 + 0.81 * ( 4.17236 - 0.77227 ) = 3.53\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 3.53\n",
      "New Q( ( 0 , 0 ) , 0 ) = 3.52634 + 0.81 * ( 4.17236 - 3.52634 ) = 4.05\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 4.35\n",
      "New Q( ( 0 , 0 ) , 3 ) = 4.34621 + 0.81 * ( 5.70045 - 4.34621 ) = 5.44\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 0 ) = 0.0\n",
      "New Q( ( 1 , 0 ) , 0 ) = 0.0 + 0.81 * ( 3.70045 - 0.0 ) = 3.0\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 0.0\n",
      "New Q( ( 1 , 0 ) , 3 ) = 0.0 + 0.81 * ( 3.70045 - 0.0 ) = 3.0\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 3.85\n",
      "New Q( ( 1 , 0 ) , 2 ) = 3.85463 + 0.81 * ( 5.22542 - 3.85463 ) = 4.96\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 0.96\n",
      "New Q( ( 0 , 0 ) , 1 ) = 0.9639 + 0.81 * ( 1.71955 - 0.9639 ) = 1.58\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 0 , 1 ) , 3 ) = -8.1\n",
      "New Q( ( 0 , 1 ) , 3 ) = -8.1 + 0.81 * ( -10.0 - -8.1 ) = -9.64\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 1.58\n",
      "New Q( ( 0 , 0 ) , 1 ) = 1.57597 + 0.81 * ( 1.71955 - 1.57597 ) = 1.69\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 1 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 2 ) = 0.0\n",
      "New Q( ( 0 , 1 ) , 2 ) = 0.0 + 0.81 * ( 0.71955 - 0.0 ) = 0.58\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 0 , 1 ) , 3 ) = -9.64\n",
      "New Q( ( 0 , 1 ) , 3 ) = -9.639 + 0.81 * ( -10.0 - -9.639 ) = -9.93\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 0.87\n",
      "New Q( ( 0 , 0 ) , 2 ) = 0.8692 + 0.81 * ( 5.22542 - 0.8692 ) = 4.4\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 4.4\n",
      "New Q( ( 0 , 0 ) , 2 ) = 4.39774 + 0.81 * ( 5.22542 - 4.39774 ) = 5.07\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 5.07\n",
      "New Q( ( 0 , 0 ) , 2 ) = 5.06816 + 0.81 * ( 5.22542 - 5.06816 ) = 5.2\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 1.69\n",
      "New Q( ( 0 , 0 ) , 1 ) = 1.69227 + 0.81 * ( 1.71955 - 1.69227 ) = 1.71\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 1 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 2 ) = 0.58\n",
      "New Q( ( 0 , 1 ) , 2 ) = 0.58283 + 0.81 * ( 0.71955 - 0.58283 ) = 0.69\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 0 ) = 0.75\n",
      "New Q( ( 0 , 1 ) , 0 ) = 0.74953 + 0.81 * ( 5.22542 - 0.74953 ) = 4.37\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 4.05\n",
      "New Q( ( 0 , 0 ) , 0 ) = 4.04962 + 0.81 * ( 5.22542 - 4.04962 ) = 5.0\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 5.44\n",
      "New Q( ( 0 , 0 ) , 3 ) = 5.44314 + 0.81 * ( 6.76637 - 5.44314 ) = 6.51\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 3.0\n",
      "New Q( ( 1 , 0 ) , 3 ) = 2.99736 + 0.81 * ( 4.76637 - 2.99736 ) = 4.43\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 4.43\n",
      "New Q( ( 1 , 0 ) , 3 ) = 4.43026 + 0.81 * ( 4.76637 - 4.43026 ) = 4.7\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 4.7\n",
      "New Q( ( 1 , 0 ) , 3 ) = 4.70251 + 0.81 * ( 4.76637 - 4.70251 ) = 4.75\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 4.75\n",
      "New Q( ( 1 , 0 ) , 3 ) = 4.75424 + 0.81 * ( 4.76637 - 4.75424 ) = 4.76\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 0 ) = 3.0\n",
      "New Q( ( 1 , 0 ) , 0 ) = 2.99736 + 0.81 * ( 4.76637 - 2.99736 ) = 4.43\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 4.76\n",
      "New Q( ( 1 , 0 ) , 3 ) = 4.76406 + 0.81 * ( 4.76637 - 4.76406 ) = 4.77\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 4.96\n",
      "New Q( ( 1 , 0 ) , 2 ) = 4.96497 + 0.81 * ( 6.25436 - 4.96497 ) = 6.01\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 5.2\n",
      "New Q( ( 0 , 0 ) , 2 ) = 5.19554 + 0.81 * ( 6.25436 - 5.19554 ) = 6.05\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 1.71\n",
      "New Q( ( 0 , 0 ) , 1 ) = 1.71436 + 0.81 * ( 5.2 - 1.71436 ) = 4.54\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 0 ) = 4.37\n",
      "New Q( ( 0 , 1 ) , 0 ) = 4.375 + 0.81 * ( 6.25436 - 4.375 ) = 5.9\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 5.0\n",
      "New Q( ( 0 , 0 ) , 0 ) = 5.00201 + 0.81 * ( 6.25436 - 5.00201 ) = 6.02\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 6.05\n",
      "New Q( ( 0 , 0 ) , 2 ) = 6.05318 + 0.81 * ( 6.25436 - 6.05318 ) = 6.22\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 6.02\n",
      "New Q( ( 0 , 0 ) , 0 ) = 6.01641 + 0.81 * ( 6.25436 - 6.01641 ) = 6.21\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 6.21\n",
      "New Q( ( 0 , 0 ) , 0 ) = 6.20915 + 0.81 * ( 6.25436 - 6.20915 ) = 6.25\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 6.25\n",
      "New Q( ( 0 , 0 ) , 0 ) = 6.24577 + 0.81 * ( 6.25436 - 6.24577 ) = 6.25\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 6.22\n",
      "New Q( ( 0 , 0 ) , 2 ) = 6.21613 + 0.81 * ( 6.25436 - 6.21613 ) = 6.25\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 6.25\n",
      "New Q( ( 0 , 0 ) , 0 ) = 6.25273 + 0.81 * ( 6.25436 - 6.25273 ) = 6.25\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 4.54\n",
      "New Q( ( 0 , 0 ) , 1 ) = 4.53773 + 0.81 * ( 6.66139 - 4.53773 ) = 6.26\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 0 ) = 5.9\n",
      "New Q( ( 0 , 1 ) , 0 ) = 5.89728 + 0.81 * ( 6.25436 - 5.89728 ) = 6.19\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 6.25\n",
      "New Q( ( 0 , 0 ) , 2 ) = 6.24709 + 0.81 * ( 6.25436 - 6.24709 ) = 6.25\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 6.51\n",
      "New Q( ( 0 , 0 ) , 3 ) = 6.51496 + 0.81 * ( 7.769 - 6.51496 ) = 7.53\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 4.77\n",
      "New Q( ( 1 , 0 ) , 3 ) = 4.76593 + 0.81 * ( 5.769 - 4.76593 ) = 5.58\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 6.01\n",
      "New Q( ( 1 , 0 ) , 2 ) = 6.00937 + 0.81 * ( 7.2295 - 6.00937 ) = 7.0\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 6.26\n",
      "New Q( ( 0 , 0 ) , 1 ) = 6.25789 + 0.81 * ( 6.93905 - 6.25789 ) = 6.81\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 0 ) = 6.19\n",
      "New Q( ( 0 , 1 ) , 0 ) = 6.18651 + 0.81 * ( 7.2295 - 6.18651 ) = 7.03\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 7.53\n",
      "New Q( ( 0 , 0 ) , 3 ) = 7.53073 + 0.81 * ( 8.71777 - 7.53073 ) = 8.49\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 7.0\n",
      "New Q( ( 1 , 0 ) , 2 ) = 6.99768 + 0.81 * ( 8.15254 - 6.99768 ) = 7.93\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 8.49\n",
      "New Q( ( 0 , 0 ) , 3 ) = 8.49223 + 0.81 * ( 9.61579 - 8.49223 ) = 9.4\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 1 , 0 ) , 1 ) = -8.1\n",
      "New Q( ( 1 , 0 ) , 1 ) = -8.1 + 0.81 * ( -10.0 - -8.1 ) = -9.64\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 6.81\n",
      "New Q( ( 0 , 0 ) , 1 ) = 6.80963 + 0.81 * ( 7.75008 - 6.80963 ) = 7.57\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 0 , 1 ) , 3 ) = -9.93\n",
      "New Q( ( 0 , 1 ) , 3 ) = -9.93141 + 0.81 * ( -10.0 - -9.93141 ) = -9.99\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 9.4\n",
      "New Q( ( 0 , 0 ) , 3 ) = 9.40232 + 0.81 * ( 9.61579 - 9.40232 ) = 9.58\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 7.93\n",
      "New Q( ( 1 , 0 ) , 2 ) = 7.93312 + 0.81 * ( 9.19222 - 7.93312 ) = 8.95\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 9.58\n",
      "New Q( ( 0 , 0 ) , 3 ) = 9.57523 + 0.81 * ( 10.59487 - 9.57523 ) = 10.4\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 0 ) = 4.43\n",
      "New Q( ( 1 , 0 ) , 0 ) = 4.43026 + 0.81 * ( 8.59487 - 4.43026 ) = 7.8\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 3 ) = 5.58\n",
      "New Q( ( 1 , 0 ) , 3 ) = 5.57842 + 0.81 * ( 8.59487 - 5.57842 ) = 8.02\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 8.95\n",
      "New Q( ( 1 , 0 ) , 2 ) = 8.95299 + 0.81 * ( 9.9851 - 8.95299 ) = 9.79\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 7.57\n",
      "New Q( ( 0 , 0 ) , 1 ) = 7.57139 + 0.81 * ( 7.75008 - 7.57139 ) = 7.72\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 2 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 1 ) = 0.0\n",
      "New Q( ( 0 , 1 ) , 1 ) = 0.0 + 0.81 * ( 0.0 - 0.0 ) = 0.0\n",
      "New State\n",
      " [[ 0.  0.  1.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  1.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 2 ) , 0 ) = 0.0\n",
      "New Q( ( 0 , 2 ) , 0 ) = 0.0 + 0.81 * ( 7.75008 - 0.0 ) = 6.28\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 0 , 1 ) , 3 ) = -9.99\n",
      "New Q( ( 0 , 1 ) , 3 ) = -9.98697 + 0.81 * ( -10.0 - -9.98697 ) = -10.0\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 10.4\n",
      "New Q( ( 0 , 0 ) , 3 ) = 10.40114 + 0.81 * ( 11.39744 - 10.40114 ) = 11.21\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 1 , 0 ) , 1 ) = -9.64\n",
      "New Q( ( 1 , 0 ) , 1 ) = -9.639 + 0.81 * ( -10.0 - -9.639 ) = -9.93\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 11.21\n",
      "New Q( ( 0 , 0 ) , 3 ) = 11.20814 + 0.81 * ( 11.39744 - 11.20814 ) = 11.36\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 1 , 0 ) , 1 ) = -9.93\n",
      "New Q( ( 1 , 0 ) , 1 ) = -9.93141 + 0.81 * ( -10.0 - -9.93141 ) = -9.99\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "############# Episode Begins ##################\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 6.25\n",
      "New Q( ( 0 , 0 ) , 0 ) = 6.25405 + 0.81 * ( 10.90701 - 6.25405 ) = 10.02\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 11.36\n",
      "New Q( ( 0 , 0 ) , 3 ) = 11.36147 + 0.81 * ( 11.39744 - 11.36147 ) = 11.39\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 1 , 0 ) , 2 ) = 9.79\n",
      "New Q( ( 1 , 0 ) , 2 ) = 9.789 + 0.81 * ( 10.93498 - 9.789 ) = 10.72\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 2 => UP\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 2 ) = 6.25\n",
      "New Q( ( 0 , 0 ) , 2 ) = 6.25298 + 0.81 * ( 10.93498 - 6.25298 ) = 10.05\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 1 reward 1 done False\n",
      "Old Q( ( 0 , 0 ) , 1 ) = 7.72\n",
      "New Q( ( 0 , 0 ) , 1 ) = 7.71613 + 0.81 * ( 7.75008 - 7.71613 ) = 7.74\n",
      "New State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 1 ) , 0 ) = 7.03\n",
      "New Q( ( 0 , 1 ) , 0 ) = 7.03133 + 0.81 * ( 10.93498 - 7.03133 ) = 10.19\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 0 => LEFT\n",
      "state2 0 reward 0 done False\n",
      "Old Q( ( 0 , 0 ) , 0 ) = 10.02\n",
      "New Q( ( 0 , 0 ) , 0 ) = 10.02295 + 0.81 * ( 10.93498 - 10.02295 ) = 10.76\n",
      "New State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Action = 3 => DOWN\n",
      "state2 3 reward 2 done False\n",
      "Old Q( ( 0 , 0 ) , 3 ) = 11.39\n",
      "New Q( ( 0 , 0 ) , 3 ) = 11.3906 + 0.81 * ( 12.28855 - 11.3906 ) = 12.12\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]] \n",
      "\n",
      "Current State\n",
      " [[ 0.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "Action = 1 => RIGHT\n",
      "state2 4 reward -10 done True\n",
      "Old Q( ( 1 , 0 ) , 1 ) = -9.99\n",
      "New Q( ( 1 , 0 ) , 1 ) = -9.98697 + 0.81 * ( -10.0 - -9.98697 ) = -10.0\n",
      "New State\n",
      " [[ 0.  0.  0.]\n",
      " [ 0.  1.  0.]] \n",
      "\n",
      "[[ 10.76169352   7.74362933  10.04539899  12.11794257]\n",
      " [ 10.19328657   0.           0.69357183  -9.9975239 ]\n",
      " [  6.27756469   0.           0.           0.        ]\n",
      " [  7.8035968   -9.9975239   10.71724268   8.02174683]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time, pickle, os\n",
    "\n",
    "env = rat()\n",
    "\n",
    "epsilon = 0.9\n",
    "total_episodes = 10\n",
    "max_steps = 100\n",
    "\n",
    "lr_rate = 0.81\n",
    "gamma = 0.96\n",
    "\n",
    "Q = np.zeros((env.stateSpace, env.actionSpace))\n",
    "    \n",
    "    \n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0,high=4)\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "def learn(state, state2, reward, action):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * np.max(Q[state2, :])\n",
    "    print('Old Q(','(',state//3,',',state%3,')',',',action,') =',round(Q[state,action],2))\n",
    "    Q[state, action] = Q[state, action] + lr_rate * (target - predict)\n",
    "    print('New Q(','(',state//3,',',state%3,')',',',action,') =',round(predict,5) ,'+',round(lr_rate,5),'*','(',round(target,5),'-',round(predict,5),') =' ,round(Q[state,action],2))\n",
    "\n",
    "# Start\n",
    "for episode in range(total_episodes):\n",
    "    actionList = ['LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    state = env.reset()\n",
    "    visualize = np.zeros((2,3))\n",
    "    visualize[state//3,state%3] = 1\n",
    "    print('############# Episode Begins ##################')\n",
    "    t = 0\n",
    "    \n",
    "    while t < max_steps:\n",
    "        \n",
    "        print('Current State\\n',visualize)\n",
    "\n",
    "        action = choose_action(state)  \n",
    "        print('Action =',action,'=>',actionList[action])\n",
    "\n",
    "        state2, reward, done = env.step(action)  \n",
    "        print('state2',state2,'reward',reward,'done',done)\n",
    "\n",
    "        learn(state, state2, reward, action)\n",
    "        \n",
    "        state = state2\n",
    "        \n",
    "        visualize = np.zeros((2,3))\n",
    "        visualize[state//3,state%3] = 1\n",
    "        print('New State\\n', visualize,'\\n')\n",
    "        #print('Q(',state,',',action,') =',round(Q[state,action],2),'\\n')\n",
    "\n",
    "        t += 1\n",
    "       \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "print(Q)\n",
    "\n",
    "#with open(\"frozenLake_qTable.pkl\", 'wb') as f:\n",
    "#    pickle.dump(Q, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA in rat env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, pickle, os\n",
    "\n",
    "env = rat()\n",
    "\n",
    "epsilon = 0.9\n",
    "# min_epsilon = 0.1\n",
    "# max_epsilon = 1.0\n",
    "# decay_rate = 0.01\n",
    "\n",
    "total_episodes = 10\n",
    "max_steps = 100\n",
    "\n",
    "lr_rate = 0.81\n",
    "gamma = 0.96\n",
    "\n",
    "Q = np.zeros((env.stateSpace, env.actionSpace))\n",
    "    \n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0,high=4)\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "def learn(state, state2, reward, action, action2):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * Q[state2, action2]\n",
    "    print('Old Q(','(',state//3,',',state%3,')',',',action,') =',round(Q[state,action],2))\n",
    "    Q[state, action] = Q[state, action] + lr_rate * (target - predict)\n",
    "    print('New Q(','(',state//3,',',state%3,')',',',action,') =',round(predict,5) ,'+',round(lr_rate,5),'*','(',round(target,5),'-',round(predict,5),') =' ,round(Q[state,action],2))\n",
    "\n",
    "\n",
    "# Start\n",
    "rewards=0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    actionList = ['LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    t = 0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    visualize = np.zeros((2,3))\n",
    "    visualize[state//3,state%3] = 1\n",
    "    print('############# Episode Begins ##################')\n",
    "    \n",
    "    action = choose_action(state)\n",
    "    \n",
    "    while t < max_steps:\n",
    "\n",
    "        print('Action = ',action, '=>', actionList[action])\n",
    "        print('Current State\\n',visualize)\n",
    "        state2, reward, done = env.step(action)\n",
    "\n",
    "        action2 = choose_action(state2)\n",
    "\n",
    "        learn(state, state2, reward, action, action2)\n",
    "\n",
    "        state = state2\n",
    "        action = action2\n",
    "        \n",
    "        visualize = np.zeros((2,3))\n",
    "        visualize[state//3,state%3] = 1\n",
    "        print('Next Action = ',action2, '=>', actionList[action2])\n",
    "        print('New State\\n', visualize,'\\n')\n",
    "        \n",
    "\n",
    "        t += 1\n",
    "        rewards+=1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    # epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode) \n",
    "    # os.system('clear')\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    \n",
    "print (\"Score over time: \", rewards/total_episodes)\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flippers Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Flipper:\n",
    "    def __init__(self,):\n",
    "        self.currState = np.ndarray.tolist(np.random.randint(0,2,size=(1,9)))[0]\n",
    "        self.currStateInt = int(\"\".join(str(x) for x in self.currState), 2)\n",
    "        self.steps = 0\n",
    "        self.endStatesInt = [7,56,73,146,292,448]\n",
    "        #[[1,0,0,1,0,0,1,0,0],[0,1,0,0,1,0,0,1,0],[0,0,1,0,0,1,0,0,1],[1,1,1,0,0,0,0,0,0],[0,0,0,1,1,1,0,0,0],[0,0,0,0,0,0,1,1,1]]\n",
    "        self.stateSpace = 512\n",
    "        self.actionSpace = 9\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.currState[action] = int(not self.currState[action])\n",
    "        self.currStateInt = int(\"\".join(str(x) for x in self.currState), 2)\n",
    "        isDone = False\n",
    "        reward = 0\n",
    "        self.steps = self.steps + 1\n",
    "    \n",
    "        if self.currStateInt in self.endStatesInt:\n",
    "            isDone = True\n",
    "            reward = pow(0.9,self.steps)\n",
    "        return(self.currStateInt,reward,isDone,self.steps)\n",
    "        \n",
    "    def reset(self,):\n",
    "        self.currState = np.ndarray.tolist(np.random.randint(0,2,size=(1,9)))[0]\n",
    "        self.currStateInt = int(\"\".join(str(x) for x in self.currState), 2)\n",
    "        self.steps = 0\n",
    "        return self.currStateInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q in Flippers env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, pickle, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = Flipper()\n",
    "\n",
    "epsilon = 0.9\n",
    "total_episodes = 1000\n",
    "max_steps = 1000\n",
    "\n",
    "lr_rate = 0.81\n",
    "gamma = 0.96\n",
    "\n",
    "Q = np.zeros((env.stateSpace, env.actionSpace))\n",
    "    \n",
    "    \n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.randint(0,high=env.actionSpace)\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "def learn(state, state2, reward, action):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * np.max(Q[state2, :])\n",
    "    #print('Old Q(','(',state//3,',',state%3,')',',',action,') =',round(Q[state,action],2))\n",
    "    Q[state, action] = Q[state, action] + lr_rate * (target - predict)\n",
    "    #print('New Q(','(',state//3,',',state%3,')',',',action,') =',round(predict,5) ,'+',round(lr_rate,5),'*','(',round(target,5),'-',round(predict,5),') =' ,round(Q[state,action],2))\n",
    "\n",
    "# Start\n",
    "\n",
    "stepsList = []\n",
    "for episode in range(total_episodes):\n",
    "    #actionList = ['LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    state = env.reset()\n",
    "    #visualize = np.zeros((2,3))\n",
    "    #visualize[state//3,state%3] = 1\n",
    "    print('############# Episode Begins ##################')\n",
    "    t = 0\n",
    "    if episode==1:\n",
    "        print(episode)\n",
    "        print(Q[145,:])\n",
    "    epsilon -= 0.001 \n",
    "    \n",
    "    while t < max_steps:\n",
    "        \n",
    "        print(Q[1,:])\n",
    "        \n",
    "        #print('Current State\\n',state)\n",
    "\n",
    "        action = choose_action(state)  \n",
    "        #print('Action =',action)\n",
    "\n",
    "        state2, reward, done, stepNum = env.step(action)  \n",
    "        #print('state2',state2,'reward',reward,'done',done)\n",
    "\n",
    "        learn(state, state2, reward, action)\n",
    "        \n",
    "        state = state2\n",
    "        \n",
    "        #visualize = np.zeros((2,3))\n",
    "        #visualize[state//3,state%3] = 1\n",
    "        #print('New State\\n', visualize,'\\n')\n",
    "        #print('Q(',state,',',action,') =',round(Q[state,action],2),'\\n')\n",
    "\n",
    "        t += 1\n",
    "       \n",
    "        if done:\n",
    "            stepsList.append(stepNum)\n",
    "            break\n",
    "\n",
    "        #time.sleep(0.1)\n",
    "        \n",
    "plt.plot(stepsList)\n",
    "#print(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
