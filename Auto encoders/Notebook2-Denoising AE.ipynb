{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, MaxPooling2D, Conv2DTranspose\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from pathlib import Path\n",
    "\n",
    "from keras import backend as keras_backend\n",
    "keras_backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_samples():\n",
    "    random_seed = 42\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Read MNIST data. We won't be using the y_train or y_test data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    pixels_per_image = np.prod(X_train.shape[1:])\n",
    "\n",
    "    # Cast values into the current floating-point type\n",
    "    X_train = keras_backend.cast_to_floatx(X_train)\n",
    "    X_test = keras_backend.cast_to_floatx(X_test)\n",
    "    \n",
    "    X_train = np.reshape(X_train, (len(X_train), 28, 28, 1)) \n",
    "    X_test = np.reshape(X_test, (len(X_test), 28, 28, 1)) \n",
    "    \n",
    "    # Normalize the range from [0,255] to [0,1]\n",
    "    X_train /= 255.\n",
    "    X_test /= 255.\n",
    "\n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_mnist(X_train, X_test, noise_factor=0.5): # add noise to the digita\n",
    "    X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape) \n",
    "    X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape) \n",
    "\n",
    "    X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "    X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
    "    return (X_train_noisy, X_test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test) = get_mnist_samples()\n",
    "(X_train_noisy, X_test_noisy) = add_noise_to_mnist(X_train, X_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_noisy_predictions_set(predictions, filename=None):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(X_test_noisy[i].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.subplot(2, 5, i+6)\n",
    "        plt.imshow(predictions[i,:,:,0].reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    file_helper.save_figure(filename+'-predictions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the autoencoder.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D((2,2,), padding='same'))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2,2), padding='same'))\n",
    "# down to 7, 7, 32 now go back up\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2,2)))\n",
    "model.add(Conv2D(1, (3,3), activation='sigmoid', padding='same'))\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 741s - loss: 0.1938 - val_loss: 0.1311\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 327s - loss: 0.1244 - val_loss: 0.1170\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 705s - loss: 0.1157 - val_loss: 0.1111\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1485s - loss: 0.1114 - val_loss: 0.1081\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1697s - loss: 0.1086 - val_loss: 0.1055\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1597s - loss: 0.1068 - val_loss: 0.1046\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1485s - loss: 0.1054 - val_loss: 0.1036\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 747s - loss: 0.1042 - val_loss: 0.1029\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 671s - loss: 0.1034 - val_loss: 0.1028\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 305s - loss: 0.1027 - val_loss: 0.1008\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.1021 - val_loss: 0.1004\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.1015 - val_loss: 0.0998\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 303s - loss: 0.1011 - val_loss: 0.1000\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.1008 - val_loss: 0.1015\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 303s - loss: 0.1004 - val_loss: 0.0992\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.1001 - val_loss: 0.0990\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0998 - val_loss: 0.0983\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0995 - val_loss: 0.0982\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0993 - val_loss: 0.0979\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0991 - val_loss: 0.0989\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0989 - val_loss: 0.0976\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 304s - loss: 0.0987 - val_loss: 0.0978\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0985 - val_loss: 0.0983\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0983 - val_loss: 0.0978\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0980 - val_loss: 0.0971\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0978 - val_loss: 0.0969\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0977 - val_loss: 0.0969\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0976 - val_loss: 0.0964\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0975 - val_loss: 0.0967\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0974 - val_loss: 0.0963\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0973 - val_loss: 0.0967\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0971 - val_loss: 0.0965\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 304s - loss: 0.0970 - val_loss: 0.0974\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0969 - val_loss: 0.0964\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0968 - val_loss: 0.0961\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0968 - val_loss: 0.0960\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0967 - val_loss: 0.0967\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0966 - val_loss: 0.0959\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0965 - val_loss: 0.0955\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0964 - val_loss: 0.0955\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0964 - val_loss: 0.0955\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0963 - val_loss: 0.0964\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0963 - val_loss: 0.0955\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 304s - loss: 0.0962 - val_loss: 0.0952\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0960 - val_loss: 0.0955\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0960 - val_loss: 0.0951\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0960 - val_loss: 0.0954\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0959 - val_loss: 0.0949\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0958 - val_loss: 0.0953\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0959 - val_loss: 0.0951\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0958 - val_loss: 0.0953\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0957 - val_loss: 0.0951\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0957 - val_loss: 0.0949\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0956 - val_loss: 0.0950\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0956 - val_loss: 0.0952\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 305s - loss: 0.0955 - val_loss: 0.0950\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 305s - loss: 0.0954 - val_loss: 0.0947\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0954 - val_loss: 0.0952\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0953 - val_loss: 0.0947\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0954 - val_loss: 0.0946\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0953 - val_loss: 0.0949\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0953 - val_loss: 0.0946\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0953 - val_loss: 0.0946\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0952 - val_loss: 0.0947\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0952 - val_loss: 0.0943\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0951 - val_loss: 0.0944\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 303s - loss: 0.0951 - val_loss: 0.0947\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0951 - val_loss: 0.0945\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0951 - val_loss: 0.0944\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0950 - val_loss: 0.0945\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0951 - val_loss: 0.0942\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0950 - val_loss: 0.0956\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0949 - val_loss: 0.0952\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0949 - val_loss: 0.0950\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0949 - val_loss: 0.0942\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0948 - val_loss: 0.0942\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0949 - val_loss: 0.0949\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0949 - val_loss: 0.0941\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 304s - loss: 0.0948 - val_loss: 0.0944\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0947 - val_loss: 0.0943\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0948 - val_loss: 0.0941\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0948 - val_loss: 0.0944\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0947 - val_loss: 0.0941\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0947 - val_loss: 0.0940\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0946 - val_loss: 0.0941\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0947 - val_loss: 0.0942\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 300s - loss: 0.0946 - val_loss: 0.0946\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0946 - val_loss: 0.0942\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0946 - val_loss: 0.0940\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0946 - val_loss: 0.0941\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0945 - val_loss: 0.0938\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0945 - val_loss: 0.0938\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 302s - loss: 0.0945 - val_loss: 0.0941\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0944 - val_loss: 0.0941\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0945 - val_loss: 0.0939\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0944 - val_loss: 0.0941\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0944 - val_loss: 0.0938\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 301s - loss: 0.0944 - val_loss: 0.0943\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "history1 = model.fit(X_train_noisy, X_train,\n",
    "                      epochs=100,\n",
    "                      batch_size=128,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(X_test_noisy, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = model1.predict(X_test_noisy)\n",
    "draw_noisy_predictions_set(predictions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the autoencoder.\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2, input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "    # down to 7, 7, 32 now go back up\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "    model.add(UpSampling2D((2,2)))\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "    model.add(UpSampling2D((2,2)))\n",
    "    model.add(Conv2D(1, (3,3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "    model.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_autoencoder2()\n",
    "weights_filename = \"NB5-Denoising-AE2\"\n",
    "np.random.seed(42)\n",
    "history2 = model2.fit(X_train_noisy, X_train,\n",
    "                      epochs=100, \n",
    "                      batch_size=128, \n",
    "                      shuffle=True, \n",
    "                      validation_data=(X_test_noisy, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(X_test_noisy)\n",
    "draw_noisy_predictions_set(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # build the autoencoder.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2, input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
    "# down to 7, 7, 32 now go back up\n",
    "model.add(Conv2DTranspose(32, (3,3), activation='relu', strides=2, padding='same'))\n",
    "model.add(Conv2DTranspose(32, (3,3), activation='relu', strides=2, padding='same'))\n",
    "model.add(Conv2D(1, (3,3), activation='sigmoid', padding='same'))\n",
    "    \n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = build_autoencoder3()\n",
    "weights_filename = \"NB5-Denoising-AE3\"\n",
    "np.random.seed(42)\n",
    "history3 = model3.fit(X_train_noisy, X_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=128,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(X_test_noisy, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model3.predict(X_test_noisy)\n",
    "draw_noisy_predictions_set(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
